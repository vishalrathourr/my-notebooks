{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f822a395",
   "metadata": {},
   "source": [
    "# <center> NLP Using Machine Learning</center>\n",
    "---\n",
    "\n",
    "This notebook demonstrates core Natural Language Processing techniques combined with Machine Learning models, including tokenization, feature extraction, and model training ‚Äî all with hands-on Python code.\n",
    "\n",
    "\n",
    "## üëã About Me\n",
    "\n",
    "Hi, I'm **Vishal Rathour**, a Data Science Enthusiast passionate about NLP and building practical AI solutions.  \n",
    "üì´ Connect with me on [LinkedIn](https://www.linkedin.com/in/vishalrathourr)  \n",
    "üîó Explore more on [GitHub](https://github.com/vishalrathourr)\n",
    "\n",
    "\n",
    "## üõ†Ô∏è Tools & Libraries\n",
    "- Python üêç  \n",
    "- NLTK / Scikit-learn  \n",
    "- Pandas / NumPy  \n",
    "- Jupyter Notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99aceb",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40701064-d77e-4d64-b847-351fb4d03e99",
   "metadata": {},
   "source": [
    "## Tokenization\n",
    "Tokenization is the process of breaking down text into smaller pieces, called tokens, which can be words, characters, or subwords.\n",
    "\n",
    "### ‚úÖ Why is Tokenization Important?\n",
    "Computers don't understand raw human language directly. Tokenization converts unstructured text into a structured format that algorithms can process.\n",
    "\n",
    "### üí° Types of Tokenization\n",
    "1. Word Tokenization ‚Äì Splits text into individual words.\n",
    "2. Character Tokenization ‚Äì Splits text into individual characters.\n",
    "3. Subword Tokenization ‚Äì Splits words into smaller meaningful parts (used in modern models like BERT or GPT).\n",
    "\n",
    "### üß† Example\n",
    "Original Text: `ChatGPT is amazing!`\n",
    "\n",
    "#### 1. Word Tokenization: \n",
    "\n",
    "`[\"ChatGPT\", \"is\", \"amazing\", \"!\"]` \n",
    "\n",
    "Each word and punctuation is treated as a separate token.\n",
    "\n",
    "#### 2. Character Tokenization: \n",
    "\n",
    "`[\"C\", \"h\", \"a\", \"t\", \"G\", \"P\", \"T\", \" \", \"i\", \"s\", \" \", \"a\", \"m\", \"a\", \"z\", \"i\", \"n\", \"g\", \"!\"]` \n",
    "\n",
    "Every character is a token, including spaces and punctuation.\n",
    "\n",
    "#### 3. Subword Tokenization (e.g., using BPE or WordPiece): \n",
    "\n",
    "`[\"Chat\", \"G\", \"PT\", \" is\", \" amazing\", \"!\"]` \n",
    "\n",
    "Words are split into smaller chunks, especially useful for rare or compound words.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d40ddf57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ChatGPT is amazing!', 'It can help you write code, explain concepts, and much more.', \"Isn't that great?\"]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "text = \"ChatGPT is amazing! It can help you write code, explain concepts, and much more. Isn't that great?\"\n",
    "\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "print(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af73ca3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ChatGPT', 'is', 'amazing', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"ChatGPT is amazing!\"\n",
    "tokens = word_tokenize(text)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e834ae0",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "- Reduces words to their root form by chopping off suffixes.\n",
    "\n",
    "### 1. Porter Stemmer\n",
    "‚úÖ Use Case:\n",
    "- Best for general English text processing tasks like information retrieval, search engines, or basic NLP pipelines.\n",
    "\n",
    "- Good balance between performance and accuracy.\n",
    "\n",
    "üìå Characteristics:\n",
    "- Rule-based and relatively conservative.\n",
    "\n",
    "- May not always produce real words.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf78d1d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fli', 'easili', 'fli', 'play', 'happili']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "words = [\"running\", \"flies\", \"easily\", \"flying\", \"played\", \"happily\"]\n",
    "\n",
    "stems = [stemmer.stem(word) for word in words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55865cc",
   "metadata": {},
   "source": [
    "### 2. Lancaster Stemmer\n",
    "‚úÖ Use Case:\n",
    "- Suitable when you value speed and want a very aggressive stemming strategy.\n",
    "\n",
    "- Good for use cases where over-stemming is acceptable, such as duplicate detection or topic clustering.\n",
    "\n",
    "üìå Characteristics:\n",
    "- More aggressive than Porter.\n",
    "\n",
    "- Often reduces words too much (over-stemming), which may distort meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2478e048",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'fli', 'easy', 'fly', 'play', 'happy']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import LancasterStemmer\n",
    "\n",
    "stemmer = LancasterStemmer()\n",
    "words = [\"running\", \"flies\", \"easily\", \"flying\", \"played\", \"happily\"]\n",
    "[stemmer.stem(w) for w in words]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5742f2",
   "metadata": {},
   "source": [
    "### 3. Snowball Stemmer (Porter2)\n",
    "‚úÖ Use Case:\n",
    "- A better choice for modern NLP tasks, offering a good balance between accuracy and aggressiveness.\n",
    "\n",
    "- Recommended for multi-language support, text normalization, and machine learning pipelines.\n",
    "\n",
    "üìå Characteristics:\n",
    "- More advanced and consistent than Porter.\n",
    "\n",
    "- Supports multiple languages (\"english\", \"french\", \"german\", etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ff7694f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['run', 'fli', 'easili', 'fli', 'play', 'happili']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "words = [\"running\", \"flies\", \"easily\", \"flying\", \"played\", \"happily\"]\n",
    "[stemmer.stem(w) for w in words]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd8ee04a",
   "metadata": {},
   "source": [
    "### 4. RegexpStemmer\n",
    "‚úÖ Use Case:\n",
    "- Custom stemming based on your own rules using regular expressions.\n",
    "\n",
    "- Best for domain-specific tasks where default stemmers don‚Äôt work well.\n",
    "\n",
    "- Useful when you want to strip predictable suffixes like ‚Äú-ing‚Äù, ‚Äú-ed‚Äù, ‚Äú-s‚Äù, etc.\n",
    "\n",
    "üìå Characteristics:\n",
    "- Allows manual control over stemming behavior.\n",
    "\n",
    "- Not intelligent‚Äîonly removes matching patterns based on your rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "01714fe6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runn', 'flie', 'play', 'talk', 'jump']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import RegexpStemmer\n",
    "\n",
    "# Remove common suffixes manually\n",
    "stemmer = RegexpStemmer('ing$|ed$|s$')\n",
    "\n",
    "words = [\"running\", \"flies\", \"played\", \"talks\", \"jumps\"]\n",
    "stems = [stemmer.stem(w) for w in words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1ae7b75e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['runn', 'play', 'read']\n"
     ]
    }
   ],
   "source": [
    "stemmer = RegexpStemmer('ing$')\n",
    "\n",
    "words = [\"running\", \"playing\", \"reading\"]\n",
    "stems = [stemmer.stem(w) for w in words]\n",
    "print(stems)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3939466",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48f457a",
   "metadata": {},
   "source": [
    "| Feature        | Stemming                    | Lemmatization                           |\n",
    "| -------------- | --------------------------- | --------------------------------------- |\n",
    "| Output         | Root form (can be non-word) | Dictionary word                         |\n",
    "| Context aware? | ‚ùå No                        | ‚úÖ Yes                                 |\n",
    "| Example        | `running ‚Üí run` (both)      | `better ‚Üí good` (only in lemmatization) |\n",
    "| POS used?      | ‚ùå Usually not               | ‚úÖ Required for accuracy               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3acd633b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['running', 'fly', 'easily', 'flying', 'played', 'happily']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "words = [\"running\", \"flies\", \"easily\", \"flying\", \"played\", \"happily\"]\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in words]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e61d49e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'fly', 'easily', 'fly', 'play', 'happily']\n"
     ]
    }
   ],
   "source": [
    "lemmas = [lemmatizer.lemmatize(word, pos='v') for word in words]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1b041d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running (v) ‚Üí run\n",
      "flies (n) ‚Üí fly\n",
      "better (a) ‚Üí good\n",
      "played (v) ‚Üí play\n",
      "children (n) ‚Üí child\n",
      "am (v) ‚Üí be\n"
     ]
    }
   ],
   "source": [
    "words = [\n",
    "    (\"running\", \"v\"),     # verb\n",
    "    (\"flies\", \"n\"),       # noun\n",
    "    (\"better\", \"a\"),      # adjective\n",
    "    (\"played\", \"v\"),\n",
    "    (\"children\", \"n\"),    # plural noun\n",
    "    (\"am\", \"v\"),          # verb (be form)\n",
    "]\n",
    "\n",
    "for word, pos in words:\n",
    "    print(f\"{word} ({pos}) ‚Üí {lemmatizer.lemmatize(word, pos)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ab56d3",
   "metadata": {},
   "source": [
    "## Parts of Speech tagging\n",
    "\n",
    "POS tagging is the process of assigning a grammatical category (noun, verb, adjective, etc.) to each word in a sentence.\n",
    "\n",
    "üìå Example Categories:\n",
    "- NN ‚Äì Noun (e.g., dog, book)\n",
    "- VB ‚Äì Verb (base form, e.g., run, play)\n",
    "- JJ ‚Äì Adjective (e.g., happy, blue)\n",
    "- RB ‚Äì Adverb (e.g., quickly, very)\n",
    "- PRP ‚Äì Personal pronoun (e.g., he, they)\n",
    "- IN ‚Äì Preposition (e.g., in, on)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0046f8",
   "metadata": {},
   "source": [
    "| Use Case                     | Benefit of POS Tagging                        |\n",
    "| ---------------------------- | --------------------------------------------- |\n",
    "| **Lemmatization**            | Uses POS to return correct base form          |\n",
    "| **Named Entity Recognition** | Helps identify proper nouns and entities      |\n",
    "| **Syntactic Parsing**        | Enables grammar-based sentence analysis       |\n",
    "| **Text classification**      | Feature for ML models (e.g., noun frequency)  |\n",
    "| **Question answering**       | Understands subject-verb-object relationships |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "edfc4c14",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import pos_tag, word_tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "642030ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"\"\"\n",
    "Natural Language Processing is a fascinating field of study in computer science.\n",
    "It deals with the interaction between computers and human language.\n",
    "Researchers are developing algorithms that can understand, interpret, and generate text just like humans.\n",
    "Applications include chatbots, translation tools, sentiment analysis, and more.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d453a241",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(paragraph)\n",
    "tagged = pos_tag(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35c79e9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Natural', 'JJ'),\n",
       " ('Language', 'NNP'),\n",
       " ('Processing', 'NNP'),\n",
       " ('is', 'VBZ'),\n",
       " ('a', 'DT'),\n",
       " ('fascinating', 'JJ'),\n",
       " ('field', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('study', 'NN'),\n",
       " ('in', 'IN')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "41dc4ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Token POS Tag\n",
      "0       Natural      JJ\n",
      "1      Language     NNP\n",
      "2    Processing     NNP\n",
      "3            is     VBZ\n",
      "4             a      DT\n",
      "5   fascinating      JJ\n",
      "6         field      NN\n",
      "7            of      IN\n",
      "8         study      NN\n",
      "9            in      IN\n",
      "10     computer      NN\n",
      "11      science      NN\n",
      "12            .       .\n",
      "13           It     PRP\n",
      "14        deals     VBZ\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(tagged, columns=[\"Token\", \"POS Tag\"])\n",
    "print(df.head(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7798dd",
   "metadata": {},
   "source": [
    "## Named Entity Recognition\n",
    "\n",
    "Named Entity Recognition (NER) is the process of identifying and classifying named entities in text into predefined categories, such as:\n",
    "\n",
    "\n",
    "| Category         | Examples                  |\n",
    "| ---------------- | ------------------------- |\n",
    "| **PERSON**       | Elon Musk, Barack Obama   |\n",
    "| **ORGANIZATION** | Google, United Nations    |\n",
    "| **LOCATION**     | Paris, Mount Everest      |\n",
    "| **DATE**         | July 4, 2025, Monday      |\n",
    "| **TIME**         | 5 p.m., noon              |\n",
    "| **MONEY**        | \\$100, 1 million euros    |\n",
    "| **PERCENT**      | 90%, 25 percent           |\n",
    "| **GPE**          | Countries, cities, states |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "48d088bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, ne_chunk\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f009bbc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Apple Inc. was founded by Steve Jobs, Steve Wozniak, and Ronald Wayne in Cupertino, California.\n",
    "In 2023, the company reported revenues of over $394 billion.\n",
    "President Joe Biden visited the Apple headquarters last summer.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "edcc6371",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('maxent_ne_chunker_tab')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d6194898",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = word_tokenize(text)\n",
    "tag_elements = pos_tag(words)\n",
    "named_entities = ne_chunk(tag_elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e2a604b-6d97-453c-90c0-55bfc92fc68f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg baseProfile=\"full\" height=\"168px\" preserveAspectRatio=\"xMidYMid meet\" style=\"font-family: times, serif; font-weight: normal; font-style: normal; font-size: 16px\" version=\"1.1\" viewBox=\"0,0,2240.0,168.0\" width=\"2240px\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:ev=\"http://www.w3.org/2001/xml-events\" xmlns:xlink=\"http://www.w3.org/1999/xlink\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">S</text></svg><svg width=\"2.85714%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Apple</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"1.42857%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5%\" x=\"2.85714%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">ORGANIZATION</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Inc.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"5.35714%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.78571%\" x=\"7.85714%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">was</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"8.75%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.21429%\" x=\"9.64286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">founded</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"11.25%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.42857%\" x=\"12.8571%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">by</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"13.5714%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.64286%\" x=\"14.2857%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"53.8462%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Steve</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.9231%\" y1=\"20px\" y2=\"48px\" /><svg width=\"46.1538%\" x=\"53.8462%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Jobs</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.9231%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"16.6071%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.07143%\" x=\"18.9286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"19.4643%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.71429%\" x=\"20%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"43.75%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Steve</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"21.875%\" y1=\"20px\" y2=\"48px\" /><svg width=\"56.25%\" x=\"43.75%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Wozniak</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"71.875%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"22.8571%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.07143%\" x=\"25.7143%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.25%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.78571%\" x=\"26.7857%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">and</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CC</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"27.6786%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5.35714%\" x=\"28.5714%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"53.3333%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Ronald</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"26.6667%\" y1=\"20px\" y2=\"48px\" /><svg width=\"46.6667%\" x=\"53.3333%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Wayne</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"76.6667%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"31.25%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.42857%\" x=\"33.9286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">in</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"34.6429%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.92857%\" x=\"35.3571%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Cupertino</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"37.3214%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.07143%\" x=\"39.2857%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"39.8214%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.28571%\" x=\"40.3571%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">GPE</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">California</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"42.5%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.07143%\" x=\"44.6429%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"45.1786%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.42857%\" x=\"45.7143%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">In</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"46.4286%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.14286%\" x=\"47.1429%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">2023</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"48.2143%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.07143%\" x=\"49.2857%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">,</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"49.8214%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.78571%\" x=\"50.3571%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"51.25%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.21429%\" x=\"52.1429%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">company</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"53.75%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.57143%\" x=\"55.3571%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">reported</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"57.1429%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.57143%\" x=\"58.9286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">revenues</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNS</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"60.7143%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.42857%\" x=\"62.5%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">of</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"63.2143%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.14286%\" x=\"63.9286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">over</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">IN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"65%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.07143%\" x=\"66.0714%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">$</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">$</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"66.6071%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.78571%\" x=\"67.1429%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">394</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"68.0357%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.21429%\" x=\"68.9286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">billion</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">CD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.5357%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.07143%\" x=\"72.1429%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"72.6786%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.92857%\" x=\"73.2143%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">President</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"75.1786%\" y1=\"20px\" y2=\"48px\" /><svg width=\"4.28571%\" x=\"77.1429%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">PERSON</text></svg><svg width=\"41.6667%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Joe</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"20.8333%\" y1=\"20px\" y2=\"48px\" /><svg width=\"58.3333%\" x=\"41.6667%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Biden</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"70.8333%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"79.2857%\" y1=\"20px\" y2=\"48px\" /><svg width=\"3.21429%\" x=\"81.4286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">visited</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">VBD</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"83.0357%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.78571%\" x=\"84.6429%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">the</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">DT</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"85.5357%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.5%\" x=\"86.4286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">Apple</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NNP</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"87.6786%\" y1=\"20px\" y2=\"48px\" /><svg width=\"5%\" x=\"88.9286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">headquarters</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"91.4286%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.14286%\" x=\"93.9286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">last</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">JJ</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"95%\" y1=\"20px\" y2=\"48px\" /><svg width=\"2.85714%\" x=\"96.0714%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">summer</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">NN</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"97.5%\" y1=\"20px\" y2=\"48px\" /><svg width=\"1.07143%\" x=\"98.9286%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg><svg width=\"100%\" x=\"0%\" y=\"48px\"><defs /><svg width=\"100%\" x=\"0\" y=\"0px\"><defs /><text text-anchor=\"middle\" x=\"50%\" y=\"16px\">.</text></svg></svg><line stroke=\"black\" x1=\"50%\" x2=\"50%\" y1=\"20px\" y2=\"48px\" /></svg><line stroke=\"black\" x1=\"50%\" x2=\"99.4643%\" y1=\"20px\" y2=\"48px\" /></svg>"
      ],
      "text/plain": [
       "Tree('S', [Tree('PERSON', [('Apple', 'NNP')]), Tree('ORGANIZATION', [('Inc.', 'NNP')]), ('was', 'VBD'), ('founded', 'VBN'), ('by', 'IN'), Tree('PERSON', [('Steve', 'NNP'), ('Jobs', 'NNP')]), (',', ','), Tree('PERSON', [('Steve', 'NNP'), ('Wozniak', 'NNP')]), (',', ','), ('and', 'CC'), Tree('PERSON', [('Ronald', 'NNP'), ('Wayne', 'NNP')]), ('in', 'IN'), Tree('GPE', [('Cupertino', 'NNP')]), (',', ','), Tree('GPE', [('California', 'NNP')]), ('.', '.'), ('In', 'IN'), ('2023', 'CD'), (',', ','), ('the', 'DT'), ('company', 'NN'), ('reported', 'VBD'), ('revenues', 'NNS'), ('of', 'IN'), ('over', 'IN'), ('$', '$'), ('394', 'CD'), ('billion', 'CD'), ('.', '.'), ('President', 'NNP'), Tree('PERSON', [('Joe', 'NNP'), ('Biden', 'NNP')]), ('visited', 'VBD'), ('the', 'DT'), ('Apple', 'NNP'), ('headquarters', 'NN'), ('last', 'JJ'), ('summer', 'NN'), ('.', '.')])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "named_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151ee457-94bb-4a7f-94a8-dc065d26e85d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a7fe02e-593b-4317-b838-469dff4e9410",
   "metadata": {},
   "source": [
    "# Text to Vectors \n",
    "### üß† Why Convert Words to Vectors?\n",
    "Machine learning models can only understand numbers, not raw text. To train models for tasks like classification, translation, sentiment analysis, etc., we need to convert words or documents into fixed-size vectors.\n",
    "\n",
    "## 1. One Hot Encoding\n",
    "### üß† What is One-Hot Encoding?\n",
    "One-Hot Encoding represents each word in a vocabulary as a binary vector:\n",
    "\n",
    "- Each word is represented by a vector the same length as the vocabulary.\n",
    "- The position corresponding to that word is 1, and all other positions are 0.\n",
    "\n",
    "#### üîç Example:\n",
    "Assume we have a vocabulary of 5 words: \\\n",
    "[\"I\", \"love\", \"NLP\", \"is\", \"fun\"]\n",
    "\n",
    "Then, one-hot encodings would be:\n",
    "| Word | One-Hot Vector   |\n",
    "| ---- | ---------------- |\n",
    "| I    | \\[1, 0, 0, 0, 0] |\n",
    "| love | \\[0, 1, 0, 0, 0] |\n",
    "| NLP  | \\[0, 0, 1, 0, 0] |\n",
    "| is   | \\[0, 0, 0, 1, 0] |\n",
    "| fun  | \\[0, 0, 0, 0, 1] |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6f6b9d01-b8d1-4521-a44b-f9f204236428",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array(['I', 'NLP', 'fun', 'is', 'love'], dtype='<U4')]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "\n",
    "words = np.array([\"I\", \"love\", \"NLP\", \"is\", \"fun\"]).reshape(-1, 1)\n",
    "\n",
    "encoder = OneHotEncoder()\n",
    "onehot = encoder.fit_transform(words)\n",
    "\n",
    "print(encoder.categories_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "355b58b1-0db2-4e8c-95d1-d1c0231068e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1.0\n",
      "  (1, 4)\t1.0\n",
      "  (2, 1)\t1.0\n",
      "  (3, 3)\t1.0\n",
      "  (4, 2)\t1.0\n"
     ]
    }
   ],
   "source": [
    "print(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84054022-4730-46d8-a6e3-14cb5e0c7770",
   "metadata": {},
   "source": [
    "#### ‚ö†Ô∏è Limitations of One-Hot Encoding\n",
    "| Limitation              | Description                                      |\n",
    "| ----------------------- | ------------------------------------------------ |\n",
    "| **High dimensionality** | If you have 10,000 words ‚Üí 10,000-length vectors |\n",
    "| **No semantics**        | All vectors are equally distant (no meaning)     |\n",
    "| **Memory inefficient**  | Mostly zeros ‚Üí sparse and wasteful               |\n",
    "\n",
    "#### ‚úÖ When to Use One-Hot Encoding\n",
    "\n",
    "| Use Case                    | Suitability                                |\n",
    "| --------------------------- | ------------------------------------------ |\n",
    "| Simple rule-based systems   | ‚úÖ Good fit                                 |\n",
    "| Small, fixed vocabularies   | ‚úÖ Efficient                                |\n",
    "| As part of embedding layers | ‚úÖ Used before training Word2Vec/BERT, etc. |\n",
    "| Large NLP models            | ‚ùå Not scalable alone                       |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9297718-b981-4be6-a1d1-c68db56fd602",
   "metadata": {},
   "source": [
    "## 2. Bag of Words (BoW)\n",
    "#### üìå Description:\n",
    "Counts how many times each word appears in a document.\n",
    "\n",
    "Ignores grammar and word order.\n",
    "\n",
    "#### ‚úÖ Use Case:\n",
    "Simple text classification tasks (e.g., spam detection, topic classification)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "f71c27e3-53bd-4924-af68-5a877782ac3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['and' 'fun' 'is' 'love' 'nlp' 'useful']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "texts = [\"I love NLP\", \"NLP is fun and useful\"]\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(texts)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9c1ee0a8-66c6-45be-a159-4cb8417feb12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 1 1 0]\n",
      " [1 1 1 0 1 1]]\n"
     ]
    }
   ],
   "source": [
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac834f5-fd7c-4e78-8238-0e0a4b9bad2a",
   "metadata": {},
   "source": [
    "### üß† What is an N-gram?\n",
    "An n-gram is a contiguous sequence of n items (usually words) from a given text or speech.\n",
    "\n",
    "- A unigram is a single word (n = 1)\n",
    "- A bigram is a pair of consecutive words (n = 2)\n",
    "- A trigram is a sequence of three words (n = 3)\n",
    "\n",
    "And so on...\n",
    "\n",
    "N-grams help preserve context and word order compared to Bag of Words.\n",
    "\n",
    "**Example: \"I love natural language processing\"**\n",
    "| N           | N-grams                                                            |\n",
    "| ----------- | ------------------------------------------------------------------ |\n",
    "| 1 (unigram) | I, love, natural, language, processing                             |\n",
    "| 2 (bigram)  | I love, love natural, natural language, language processing        |\n",
    "| 3 (trigram) | I love natural, love natural language, natural language processing |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "65d9466e-a873-4b8c-b63d-db39599cc953",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N-gram Vocabulary: ['language' 'language processing' 'love' 'love natural' 'natural'\n",
      " 'natural language' 'processing']\n",
      "N-gram Matrix:\n",
      " [[1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "text = [\"I love natural language processing\"]\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))  # unigrams and bigrams\n",
    "X = vectorizer.fit_transform(text)\n",
    "\n",
    "print(\"N-gram Vocabulary:\", vectorizer.get_feature_names_out())\n",
    "print(\"N-gram Matrix:\\n\", X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aef793b1-deee-4d17-834f-dcbf76ee0ff2",
   "metadata": {},
   "source": [
    "#### ‚úÖ Why Use N-grams?\n",
    "| Use Case            | Benefit                                   |\n",
    "| ------------------- | ----------------------------------------- |\n",
    "| Language modeling   | Predict the next word based on previous n |\n",
    "| Text classification | Capture phrases (e.g., \"not good\")        |\n",
    "| Sentiment analysis  | Understand multi-word expressions         |\n",
    "| Spelling correction | Match common n-gram sequences             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7041819f-bd25-4bd9-9ddd-f39a352d6ae3",
   "metadata": {},
   "source": [
    "## 3. TF-IDF\n",
    "\n",
    "TF-IDF stands for:\n",
    "\n",
    "- **TF** ‚Äì Term Frequency: how often a word appears in a document\n",
    "- **IDF** ‚Äì Inverse Document Frequency: how unique or rare a word is across all documents\n",
    "\n",
    "Together, TF-IDF measures how important a word is to a specific document in a collection. \\\n",
    "\n",
    "**Term Frequency (TF)**  \n",
    "$$\n",
    "\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n",
    "$$\n",
    "\n",
    "**Inverse Document Frequency (IDF)**  \n",
    "$$\n",
    "\\text{IDF}(t) = \\log\\left(\\frac{N}{1 + df(t)}\\right)\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "\n",
    "- N: total number of documents\n",
    "\n",
    "- df(t): number of documents containing term t\n",
    "\n",
    "**TF-IDF**  \n",
    "$$\n",
    "\\text{TF-IDF}(t, d) = \\text{TF}(t, d) \\times \\text{IDF}(t)\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c499fb5",
   "metadata": {},
   "source": [
    "Suppose we have 3 documents:\n",
    "\n",
    "Doc1: \"I love NLP\" \\\n",
    "Doc2: \"NLP is fun\" \\\n",
    "Doc3: \"I love fun\" \\\n",
    "Let‚Äôs calculate TF-IDF for the term \"love\": \\\n",
    "\n",
    "\n",
    "- Appears in Doc1 and Doc3\n",
    "- Total documents = 3 ‚Üí ùëÅ=3\n",
    "- Document Frequency ùëëùëì(\"ùëôùëúùë£ùëí\")=2\n",
    "\n",
    "So,\n",
    "\n",
    "$ IDF('ùëôùëúùë£ùëí')=\\log\\left(\\frac{3}{1 + 2}\\right)=0 $ \\\n",
    "\\\n",
    "**Meaning:** \"love\" is too common ‚Üí has low importance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c4d7fc52-42b4-40a5-be66-b791c148a5bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['fun' 'is' 'love' 'nlp']\n",
      "[[0.         0.         0.70710678 0.70710678]\n",
      " [0.51785612 0.68091856 0.         0.51785612]\n",
      " [0.70710678 0.         0.70710678 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "docs = [\"I love NLP\", \"NLP is fun\", \"I love fun\"]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(docs)\n",
    "\n",
    "print(vectorizer.get_feature_names_out())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6339085",
   "metadata": {},
   "source": [
    "#### ‚úÖ Why Use TF-IDF?\n",
    "| Feature                         | Benefit                                                        |\n",
    "| ------------------------------- | -------------------------------------------------------------- |\n",
    "| Weights important words         | Down-weights common but uninformative words (like ‚Äúthe‚Äù, ‚Äúis‚Äù) |\n",
    "| Improves text classification    | Better than raw counts (BoW)                                   |\n",
    "| Works well for document ranking | Used in search engines like Google                             |\n",
    "\n",
    "#### ‚ùå Limitations\n",
    "| Limitation           | Description                                    |\n",
    "| -------------------- | ---------------------------------------------- |\n",
    "| No semantic meaning  | ‚Äúlove‚Äù and ‚Äúlike‚Äù are unrelated numerically    |\n",
    "| Still sparse vectors | May be inefficient for very large vocabularies |\n",
    "| Ignores word order   | Can‚Äôt detect phrases or grammar                |\n",
    "\n",
    "#### üìù Summary\n",
    "| Term       | Meaning                               |\n",
    "| ---------- | ------------------------------------- |\n",
    "| **TF**     | How often a word occurs in a document |\n",
    "| **IDF**    | How rare the word is across all docs  |\n",
    "| **TF-IDF** | Importance of a word in a document    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb0da82",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "Word embeddings are dense vector representations of words in a continuous vector space, where similar words are mapped to similar vectors.\n",
    "\n",
    "`vector(\"king\") - vector(\"man\") + vector(\"woman\") ‚âà vector(\"queen\")`\n",
    "\n",
    "\n",
    "Word embedding is a way to turn words into numbers, so a computer can understand them ‚Äî but not just any numbers.\n",
    "\n",
    "It represents each word as a list of numbers (a vector) where:\n",
    "- Similar words (like \"king\" and \"queen\") get similar numbers\n",
    "- Different words (like \"apple\" and \"car\") get very different numbers\n",
    "\n",
    "So the computer can figure out which words are related and how closely.\n",
    "| Word  | Embedding (vector example) |\n",
    "| ----- | -------------------------- |\n",
    "| king  | \\[0.2, 0.5, -0.3]          |\n",
    "| queen | \\[0.2, 0.6, -0.3]          |\n",
    "| apple | \\[-0.7, 0.1, 0.4]          |\n",
    "\n",
    "\"king\" and \"queen\" have similar vectors ‚Üí so the computer knows they are related.\n",
    "\n",
    "#### ü§î Why Do We Need Word Embeddings?\n",
    "| Problem with Traditional Approaches | Word Embeddings Solve                       |\n",
    "| ----------------------------------- | ------------------------------------------- |\n",
    "| One-hot vectors are huge & sparse   | Embeddings are dense & compact              |\n",
    "| No meaning or similarity captured   | Embeddings group similar meanings together  |\n",
    "| Cannot generalize across contexts   | Embeddings help capture word usage patterns |\n",
    "\n",
    "#### üí° Key Idea\n",
    "Each word is represented as a vector of real numbers (e.g., 100‚Äì300 dimensions), trained so that words used in similar contexts have similar vectors.\n",
    "\n",
    "#### üõ† Common Word Embedding Models\n",
    "| Model          | Description                                                          |\n",
    "| -------------- | -------------------------------------------------------------------- |\n",
    "| **Word2Vec**   | Predicts a word from its context (or vice versa)                     |\n",
    "| **GloVe**      | Builds word vectors using matrix factorization of word co-occurrence |\n",
    "| **FastText**   | Includes subword information (good for misspellings and rare words)  |\n",
    "| **ELMo, BERT** | Contextual embeddings (meaning changes based on sentence context)    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db5bf63",
   "metadata": {},
   "source": [
    "### üß† What is Word2Vec?\n",
    "Word2Vec is a method to convert words into vectors so that:\n",
    "\n",
    "- Similar words have similar vectors\n",
    "- It captures meaning and context of words\n",
    "\n",
    "It was introduced by Google in 2013 and is trained using a neural network.\n",
    "\n",
    "#### üîÅ How It Works \n",
    "Word2Vec trains on a text corpus and learns word relationships.\n",
    "\n",
    "Two training methods:\n",
    "\n",
    "##### 1. CBOW (Continuous Bag of Words)\n",
    "**Concept:**\n",
    "CBOW predicts the target word using its context words (surrounding words).\n",
    "\n",
    "- Input: Context (surrounding words)\n",
    "- Output: Target word (the word in the middle)\n",
    "\n",
    "**Example:**\n",
    "Given the sentence:\n",
    "\"The cat sits on the mat\"\n",
    "\n",
    "For a context window of size 2:\n",
    "Context: [\"The\", \"sits\"] ‚Üí Target: \"cat\"\n",
    "\n",
    "CBOW tries to learn the representation such that, given surrounding words, it can predict the central word.\n",
    "\n",
    "##### 2. Skip-Gram\n",
    "**Concept:**\n",
    "Skip-Gram does the reverse. It predicts context words from the target word.\n",
    "- Input: Target word\n",
    "- Output: Context (surrounding words)\n",
    "\n",
    "**Example:**\n",
    "Given the same sentence:\n",
    "\"The cat sits on the mat\"\n",
    "\n",
    "Target: \"cat\" ‚Üí Context: [\"The\", \"sits\"]\n",
    "\n",
    "Skip-Gram tries to learn word representations such that, given a word, it can predict its context. \n",
    "\n",
    "##### CBOW vs Skip-Gram\n",
    "| Feature               | CBOW             | Skip-Gram             |\n",
    "| --------------------- | ---------------- | --------------------- |\n",
    "| Direction             | Context ‚Üí Target | Target ‚Üí Context      |\n",
    "| Performance           | Faster to train  | Slower to train       |\n",
    "| Accuracy (Rare Words) | Lower            | Better for rare words |\n",
    "\n",
    "**Conclusion**\n",
    "- CBOW is faster and works better for frequent words.\n",
    "- Skip-Gram is better for rare words and provides finer representations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "d4b7e32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "09d02bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['the', 'cat', 'sits', 'on', 'the', 'mat', '.', 'the', 'dog', 'plays', 'with', 'the', 'cat', '.']\n",
      "\n",
      "CBOW Vector for 'cat':\n",
      " [-0.01723935  0.00733214  0.0103783   0.01148358  0.01493548 -0.01233377\n",
      "  0.00221044  0.01209513 -0.00568189 -0.01234571 -0.00082144 -0.01673858\n",
      " -0.01119803  0.01420926  0.00670669  0.01445003  0.0136016   0.01506361\n",
      " -0.00758013 -0.00112471  0.00469524 -0.00903911  0.01677665 -0.01971816\n",
      "  0.01353099  0.00582779 -0.00986385  0.00879751 -0.00348056  0.01342362\n",
      "  0.01993087 -0.00872648 -0.00120027 -0.01139181  0.00769979  0.00557291\n",
      "  0.01378207  0.0122029   0.0190773   0.01854664  0.01579493 -0.01397864\n",
      " -0.01831192 -0.00071005 -0.00619883  0.01578961  0.01187746 -0.00309191\n",
      "  0.00302099  0.00357986]\n",
      "\n",
      "Skip-Gram Vector for 'cat':\n",
      " [-0.01723935  0.00733214  0.0103783   0.01148358  0.01493548 -0.01233377\n",
      "  0.00221044  0.01209513 -0.00568189 -0.01234571 -0.00082144 -0.01673858\n",
      " -0.01119803  0.01420926  0.00670669  0.01445003  0.0136016   0.01506361\n",
      " -0.00758013 -0.00112471  0.00469524 -0.00903911  0.01677665 -0.01971816\n",
      "  0.01353099  0.00582779 -0.00986385  0.00879751 -0.00348056  0.01342362\n",
      "  0.01993087 -0.00872648 -0.00120027 -0.01139181  0.00769979  0.00557291\n",
      "  0.01378207  0.0122029   0.0190773   0.01854664  0.01579493 -0.01397864\n",
      " -0.01831192 -0.00071005 -0.00619883  0.01578961  0.01187746 -0.00309191\n",
      "  0.00302099  0.00357986]\n",
      "\n",
      "CBOW similar to 'cat': [('plays', 0.16573308408260345), ('sits', 0.13941362500190735), ('the', 0.12668678164482117), ('mat', 0.08872270584106445), ('.', 0.011084318161010742), ('dog', -0.027830608189105988), ('with', -0.15517376363277435), ('on', -0.21871204674243927)]\n",
      "Skip-Gram similar to 'cat': [('plays', 0.16573308408260345), ('sits', 0.13941362500190735), ('the', 0.12668678164482117), ('mat', 0.08872270584106445), ('.', 0.011084318161010742), ('dog', -0.027830608189105988), ('with', -0.15517376363277435), ('on', -0.21871204674243927)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "# Sample corpus\n",
    "text = \"The cat sits on the mat. The dog plays with the cat.\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = word_tokenize(text.lower())\n",
    "print(\"Tokens:\", tokens)\n",
    "\n",
    "# Create a list of sentences for Word2Vec (list of list of tokens)\n",
    "data = [tokens]\n",
    "\n",
    "# CBOW Model (sg=0)\n",
    "cbow_model = Word2Vec(sentences=data, vector_size=50, window=2, min_count=1, sg=0)\n",
    "\n",
    "# Skip-Gram Model (sg=1)\n",
    "skipgram_model = Word2Vec(sentences=data, vector_size=50, window=2, min_count=1, sg=1)\n",
    "\n",
    "# Check vector of a word\n",
    "print(\"\\nCBOW Vector for 'cat':\\n\", cbow_model.wv['cat'])\n",
    "print(\"\\nSkip-Gram Vector for 'cat':\\n\", skipgram_model.wv['cat'])\n",
    "\n",
    "# Find similar words\n",
    "print(\"\\nCBOW similar to 'cat':\", cbow_model.wv.most_similar('cat'))\n",
    "print(\"Skip-Gram similar to 'cat':\", skipgram_model.wv.most_similar('cat'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c745afd",
   "metadata": {},
   "source": [
    "# <center> Practical Implementation</center>\n",
    "## <center> Spam vs Ham Classification</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fce12e0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: C:\\Users\\visha\\.cache\\kagglehub\\datasets\\bagavathypriya\\spam-ham-dataset\\versions\\1\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"bagavathypriya/spam-ham-dataset\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d47e58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "df = pd.read_csv(r\"C:\\Users\\visha\\.cache\\kagglehub\\datasets\\bagavathypriya\\spam-ham-dataset\\versions\\1\\spamhamdata.csv\", sep=\"\\t\", encoding=\"utf-8\", names=[\"label\", \"text\"])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "73daf78c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               text\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c408e983",
   "metadata": {},
   "source": [
    "### 1. Using BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "dc8261f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\visha\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Data cleaning and preprocessing\n",
    "import re\n",
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "295643b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "dca112f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = []\n",
    "\n",
    "for i in range(0, len(df)):\n",
    "    review = re.sub('[^a-zA-Z]', ' ', df['text'][i])\n",
    "    review = review.lower()\n",
    "    review = review.split()\n",
    "    review = [ps.stem(word) for word in review if not word in set(stopwords.words('english'))]\n",
    "    review = ' '.join(review)\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71d734ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['go jurong point crazi avail bugi n great world la e buffet cine got amor wat',\n",
       " 'ok lar joke wif u oni',\n",
       " 'free entri wkli comp win fa cup final tkt st may text fa receiv entri question std txt rate c appli',\n",
       " 'u dun say earli hor u c alreadi say',\n",
       " 'nah think goe usf live around though']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "18d97b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Bag of Words model\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features=5000, ngram_range=(1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "7e8c01fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# independent variable\n",
    "X = cv.fit_transform(corpus).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fa2aa1b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.set_printoptions(edgeitems=30, linewidth=100000, formatter=dict(float=lambda x: \"%0.3f\" % x))\n",
    "X[:5, :10]  # Display first 5 rows and first 10 columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1103603f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5572, 5000)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "70f558b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "be5993ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output feature \n",
    "y = pd.get_dummies(df['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d4b1343a",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.iloc[:, 1].values # 0 for ham, 1 for spam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "626e5399",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "7e74ea83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[962   4]\n",
      " [  8 141]]\n",
      "Accuracy: 0.989237668161435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      1.00      0.99       966\n",
      "        spam       0.97      0.95      0.96       149\n",
      "\n",
      "    accuracy                           0.99      1115\n",
      "   macro avg       0.98      0.97      0.98      1115\n",
      "weighted avg       0.99      0.99      0.99      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Create the classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Predict the test set results\n",
    "y_pred = classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Display the confusion matrix\n",
    "print(\"Confusion Matrix:\\n\", cm)    \n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "\n",
    "# Display the classification report\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921a8a2c",
   "metadata": {},
   "source": [
    "### 2. Using TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f6e2d0a3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "       [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "       [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "       [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000],\n",
       "       [0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000, 0.000]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Creating the tfidf model\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1, 2))\n",
    "X = tfidf.fit_transform(corpus).toarray()\n",
    "X[:5, :10]  # Display first 5 rows and first 10 columns\n",
    "# X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9c316981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tfidf.vocabulary_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "12b73d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      " [[966   0]\n",
      " [ 24 125]]\n",
      "Accuracy: 0.97847533632287\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      1.00      0.99       966\n",
      "        spam       1.00      0.84      0.91       149\n",
      "\n",
      "    accuracy                           0.98      1115\n",
      "   macro avg       0.99      0.92      0.95      1115\n",
      "weighted avg       0.98      0.98      0.98      1115\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "tfidf_classifier = MultinomialNB()\n",
    "tfidf_classifier.fit(X_train, y_train)\n",
    "y_pred = tfidf_classifier.predict(X_test)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"Confusion Matrix:\\n\", cm)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(classification_report(y_test, y_pred, target_names=['ham', 'spam']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4879b465",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
